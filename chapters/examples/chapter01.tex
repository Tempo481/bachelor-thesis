\chapter{Einleitung}
\label{ch:intro}
OAuth2 als Spezifikation kann unter anderem dafür genutzt werden \ac{HTTP}-Schnittstellen zu sichern. Es ist heutzutage praktisch Standard und hat eine Vielzahl von Einsatzzwecken, wie neben der Sicherung von Schnittstellen auch die Authentifizierung. Die meisten Konzerne setzen Implementierungen dieser Spezifikation ein, so auch beispielsweise Microsoft \citep{microsoftoauth2:2021:07}. Ein typischer Ablauf in OAuth2 ist folgendermaßen zu beschreiben: Ein Nutzer authentifiziert sich und daraufhin erhält ein Client von einem Autorisationsserver einen Token, mit dem er auf gesicherte Schnittstellen eines Ressource Servers zugreifen kann. Hierbei ist zu erwähnen, dass nutzerspezifische Attribute in den Token gemappt werden. Dies kann beispielsweise der Vor-und-Nachname sein, die E-Mail-Adresse, die Abteilung, in der der Nutzer arbeitet, sowie etwaige Rollen und Gruppenzugehörigkeiten. In dem Ressource Server ist es oftmals notwendig nach diesen Nutzerattributen zu autorisieren. Beispielsweise kann die Anforderung bestehen, dass ein Nutzer mit der Rolle ROLE\_ADMIN oder ein Nutzer der in der Abteilung Human Ressources arbeitet, Admin-Privilegien erhalten soll, das heißt auf Schnittstellen zugreifen kann, die voraussetzen das diese Attribute in dem Token vorhanden sind.\smallskip

Grundsätzlich lassen sich diese Zugriffsrichtlinien in dem Ressource Server selbst implementieren. Allerdings haben wir es in der Praxis in Firmen mit einer Vielzahl von Applikationen, das heißt Ressource Servern, zu tun die alle verschiedene Zugriffsrichtlinien haben und zudem mit verschiedenen Programmiersprachen und Frameworks geschrieben sind. Dies kann vor allem bei komplexen und sich häufig ändernden Zugriffsrichtlinien zum einen zu einem hohen Wartungsaufwand führen und zum anderen auch zu Sicherheitsrisiken führen, indem beispielsweise vergessen wird bei einer Applikation eine Schnittstelle nur für Nutzer mit Admin-Privilegien freizugeben oder eine falsche Annotation verwendet wird. 
Unter anderem aus diesem Grund wurde Projekt \emph{Open Policy Agent} (OPA) entwickelt. Es entkoppelt die Zugriffskontrolle von dem Ressource Server als externes Programm in einer leicht verständlichen Programmiersprache, die ausschließlich für die Zugriffskontrolle zuständig ist. Das bedeutet, wenn ein Ressource Server von einem Client eine HTTP-Anfrage mit einem validen Token erhält, sendet er diese Anfrage mit dem Token an den OPA-Service. In dem OPA-Service sind Richtlinien zur Zugriffskontrolle in der Programmiersprache Rego hinterlegt. Beispielsweise kann in dem OPA-Service eine Zugriffskontrolle hinterlegt sein, die besagt, dass auf die HTTP-GET-Schnittstelle mit dem Pfad /secretData nur Zugriff gewährleistet werden darf, wenn in dem Token das Schlüssel-Wert-Paar roles: ROLE\_ADMIN hinterlegt ist. Falls dem so ist, würde der OPA-Service eine Zugriffserlaubnis an den Ressource Server zurücksenden, der dann wiederum dem Client entsprechend die Daten von der GET-Schnittstelle /secretData zusendet, da ja OPA evaluiert hat, dass dieser Token die nötigen Befugnisse verfügt und damit der Nutzer ordnungsgemäß autorisiert ist diese Schnittstelle aufzurufen. Ansonsten würde \ac{OPA} den Zugriff verweigern und dementsprechend würde der Ressource Server die HTTP-GET-Anfrage nicht beantworten und stattdessen eine HTTP-401-Meldung (Unauthorized) dem Client zusenden. Ein weiterer Vorteil der Entkopplung von Autorisierung ist aus Sicht des Software-Engineering die Trennung der Zuständigkeiten (Separation of Concerns). 

%
% Section: Motivation
%
\section{Motivation}
\label{sec:intro:motivation}
%\graffito{Note: The content of this chapter is just some dummy text. It is not a real language.}
Ein Nachteil dieser Entkopplung speziell in OAuth2 Systemen ist der, dass der Ressource Server jedes Mal bei einkommenden HTTP-Anfragen den OPA-Service die HTTP-Anfrage inklusive des Tokens zusenden muss, dieser dann den Token dekodieren, parsen und dann schlussendlich anhand dessen eine Zugriffsentscheidung evaluieren und dem Ressource Server zusenden muss. Das heißt es entsteht ein nicht zu vernachlässigbarer zusätzlicher Kommunikationsverkehr, was zu Performanceproblemen führen kann. Bei einer hohen Last auf Schnittstellen, werden diese in der Regel horizontal skaliert. Das bedeutet, falls eine Schnittstelle aufgrund zu hoher Last nicht mehr in akzeptabler Zeit HTTP-Anfragen beantworten kann, wird eine neue Instanz dieser Applikation auf einem zweiten Host erstellt und ein Loadbalancer sorgt dafür, dass die Last gleichmäßig verteilt wird und dadurch die Antwortzeiten möglichst niedrig gehalten werden. Diese Metrik wird auch die Response Time genannt, also die Zeit vor dem Senden einer HTTP-Anfrage an den Server bis zum Eintreffen des letzten Bytes der Antwort des Servers \citep{jmeterglossary:2021}. 
Die Entwickler von Open Policy Agent geben an, dass anhand von durchgeführten Benchmarks Zugriffsentscheidungen in der Regel nur Rechenzeit im Bereich von lediglich unter einer Millisekunde benötigen \citep{opaperformance:2021:07}. In diesen Benchmarks wurden allerdings weder zu dekodierenden Tokens verwendet noch wurde auf die umso wichtigere Latenz eingegangen: Nämlich die Zeit, die gebraucht wird, wenn ein Client eine HTTP-Anfrage an einen Ressource Server sendet, dieser dann die Anfrage an den OPA-Service zur Evaluierung einer Zugriffsentscheidung sendet und dann schlussendlich der Ressource Server basierend auf der Zugriffsentscheidung des OPA-Services dem Client antwortet. Zudem ist ungewiss, wie sich eine externe Zugirffskontrolle mit OPA unter Last im Vergleich zur Zugriffskontrolle in dem Ressource Server selbst verhält. Potenziell könnten nicht vertretbar hohe Latenzen entstehen, die entweder durch unter Umständen teure horizontale Skalierung entgegnet werden müssen oder das Nutzererlebnis durch hohe Antwortzeiten leidet.

%
% Section: Ziele
%
\section{Ziel der Arbeit und Ergebnisse}
\label{sec:intro:goal}
Um den Einfluss von externer Zugriffskontrolle auf die Performanz in OAuth2 Systemen zu untersuchen, wurden zwei Ressource Server implementiert. In dem einen System wird die Zugriffskontrolle in dem Ressource Server selbst gehandhabt und in dem anderen wird sie entkoppelt durch den Open Policy Agent. 
Als Tool zur Generierung von Last und dem Messen und Protokollieren der Response Time wurde Apache JMeter verwendet. Neben dem Last-Test wurden zwei weitere Testpläne in JMeter erstellt. Einmal ein Test zur Skalierbarkeit und ein Stress-Test. Bei dem Skalierbarkeitstest wird nicht eine gleichbleibende Last erzeugt, sondern es wird periodisch Last durch hinzukommende Threads erhöht, um zu sehen, inwiefern sich die Response Time bei hinzukommender Last erhöht. Diese Art des Tests ist aus wirtschaftlicher Sicht sinnvoll, denn anhand dessen lässt sich herleiten wie sehr sich Antwortzeiten bei hinzukommender Anzahl von Nutzern erhöht, sodass vorausgesehen werden kann, wann Ressource Server skaliert werden muss, damit die Anfragen von Nutzern akzeptabel niedrige Antwortzeiten haben. \smallskip

Bei dem Stresstest geht es darum, eine hohe Last auf den Ressource Server zu generieren mit dem Ziel herauszufinden, wie viele Nutzer der Ressource Server gleichzeitig bedienen kann bis entweder der Ressource Server vollkommen unerreichbar ist oder die Antwortzeiten unakzeptabel hoch werden. Diese drei Testpläne, Last-, Skalierbarkeit-und-Stresstest wurden auf dem Ressource Server mit und ohne OPA-Service durchgeführt und die Resultate verglichen.\smallskip

Bei zehn gleichzeitigen Threads stellte es sich heraus, dass die Latenzen bei dem System mit OPA durchschnittlich mehr als dreimal so hoch ausfielen. Zudem skaliert das System mit \ac*{OPA} deutlich schlechter, denn bei ansteigender Last auf bis zu 100 Nutzer stiegen die Latenzen linear, während bei dem System ohne OPA diese konstant niedrig blieben. Zudem war \ac*{OPA} unter starker Last zeitweise nicht erreichbar für den Ressource Server. Es konnte also ein signifikanter Performancenachteil nachgewiesen werden und die Hypothese, dass durch den zusätzlichen Kommunikationsaufwand der zwischen Server und \ac*{OPA} auftritt, ein negativer Einfluss auf die Performance entsteht, konnte bestätigt werden.

%
% Section: Struktur der Arbeit
%
\section{Gliederung}
\label{sec:intro:structure}
Zunächst werden die benötigten technischen Grundlagen zusammengetragen. Darauf folgt ein Kapitel, in dem die jeweiligen Ressource Server und die Performancetests, die mit Apache JMeter ausgeführt werden, beschrieben werden. In dem darauffolgenden Kapitel werden die Ergebnisse der Performancetests ausgewertet. Dann, in dem vorletzten Kapitel, folgt das Kapitel Stand der Technik, indem verwandte Arbeiten diskutiert werden. Zum Schluss folgt eine Zusammenfassung. 
